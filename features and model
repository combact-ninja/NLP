from transformers import BertModel, BertTokenizer

# Load pre-trained BERT models and tokenizers
encoder_model_name = 'bert-base-uncased'
decoder_model_name = 'bert-base-uncased'
encoder_tokenizer = BertTokenizer.from_pretrained(encoder_model_name)
decoder_tokenizer = BertTokenizer.from_pretrained(decoder_model_name)
encoder_model = BertModel.from_pretrained(encoder_model_name)
decoder_model = BertModel.from_pretrained(decoder_model_name)


import torch
import torch.nn as nn

class SummarizationModel(nn.Module):
    def __init__(self, encoder, decoder):
        super(SummarizationModel, self).__init__()
        self.encoder = encoder
        self.decoder = decoder
        self.cross_attention = nn.MultiheadAttention(embed_dim=encoder.config.hidden_size, num_heads=8)

    def forward(self, input_ids, decoder_input_ids, encoder_attention_mask=None, decoder_attention_mask=None):
        # Get encoder output
        encoder_output = self.encoder(input_ids=input_ids, attention_mask=encoder_attention_mask)[0]

        # Get decoder output
        decoder_output = self.decoder(input_ids=decoder_input_ids, attention_mask=decoder_attention_mask)[0]

        # Apply cross-attention from encoder to decoder
        cross_attention_output, _ = self.cross_attention(decoder_output, encoder_output, encoder_output)

        return cross_attention_output
# Example input
input_text = "Your input text here"
target_text = "Your target summary here"
inputs = encoder_tokenizer(input_text, return_tensors='pt')
targets = decoder_tokenizer(target_text, return_tensors='pt')
input_ids = inputs['input_ids']
decoder_input_ids = targets['input_ids']
encoder_attention_mask = inputs['attention_mask']
decoder_attention_mask = targets['attention_mask']

# Create model instance
model = SummarizationModel(encoder_model, decoder_model)

# Get model output
output = model(input_ids=input_ids, decoder_input_ids=decoder_input_ids,
               encoder_attention_mask=encoder_attention_mask, decoder_attention_mask=decoder_attention_mask)

print(output.shape)  # Shape of the output tensor


############ Modified Model
import torch
import torch.nn as nn
import torch.nn.functional as F

class ModifiedAbstractiveAttention(nn.Module):
    def __init__(self, hidden_size):
        super(ModifiedAbstractiveAttention, self).__init__()
        self.hidden_size = hidden_size
        self.linear = nn.Linear(hidden_size, hidden_size)
        self.softmax = nn.Softmax(dim=-1)

    def forward(self, query, key, value, mask=None):
        # Calculate attention scores
        scores = torch.matmul(query, key.transpose(-2, -1))
        scores = scores / torch.sqrt(torch.tensor(self.hidden_size, dtype=torch.float32))

        # Apply mask (if provided)
        if mask is not None:
            scores = scores.masked_fill(mask == 0, -1e9)

        # Apply softmax to get attention weights
        weights = self.softmax(scores)

        # Apply attention weights to value
        output = torch.matmul(weights, value)

        return output, weights
class SummarizationModel(nn.Module):
    def __init__(self, encoder, decoder):
        super(SummarizationModel, self).__init__()
        self.encoder = encoder
        self.decoder = decoder
        self.cross_attention = nn.MultiheadAttention(embed_dim=encoder.config.hidden_size, num_heads=8)
        self.attention = ModifiedAbstractiveAttention(hidden_size=encoder.config.hidden_size)

    def forward(self, input_ids, decoder_input_ids, encoder_attention_mask=None, decoder_attention_mask=None):
        # Get encoder output
        encoder_output = self.encoder(input_ids=input_ids, attention_mask=encoder_attention_mask)[0]

        # Get decoder output
        decoder_output = self.decoder(input_ids=decoder_input_ids, attention_mask=decoder_attention_mask)[0]

        # Calculate query, key, and value for encoder and decoder
        encoder_query = self.encoder(input_ids=input_ids, attention_mask=encoder_attention_mask)[0]
        encoder_key = encoder_output
        encoder_value = encoder_output
        decoder_query = self.decoder(input_ids=decoder_input_ids, attention_mask=decoder_attention_mask)[0]

        # Apply cross-attention with modified abstractive attention
        cross_attention_output, _ = self.cross_attention(query=decoder_query, key=encoder_key, value=encoder_value)

        # Apply modified abstractive attention
        modified_attention_output, _ = self.attention(query=decoder_query, key=cross_attention_output, value=cross_attention_output)

        return modified_attention_output


##### features
#name entities
import nltk
from nltk.tokenize import word_tokenize
from nltk.tag import pos_tag
from nltk.chunk import ne_chunk

# Download NLTK resources
nltk.download('punkt')
nltk.download('averaged_perceptron_tagger')
nltk.download('maxent_ne_chunker')
nltk.download('words')

# Sample text
text = "Barack Obama was born in Hawaii on August 4, 1961. He was the 44th President of the United States."

# Tokenize the text
tokens = word_tokenize(text)

# Part-of-speech tagging
pos_tags = pos_tag(tokens)

# Named entity recognition
ne_tags = ne_chunk(pos_tags)

# Extract named entities
named_entities = []
for chunk in ne_tags:
    if hasattr(chunk, 'label'):
        named_entities.append(' '.join(c[0] for c in chunk.leaves()))

print("Named Entities:")
for entity in named_entities:
    print(entity)
#### POS-tag
import nltk
from nltk.tokenize import word_tokenize
from nltk.tag import pos_tag

# Sample text
text = "Barack Obama was born in Hawaii on August 4, 1961. He was the 44th President of the United States."

# Tokenize the text
tokens = word_tokenize(text)

# Perform POS tagging
pos_tags = pos_tag(tokens)

# Display POS tags
print("POS Tags:")
for token, pos_tag in pos_tags:
    print(f"{token}: {pos_tag}")
### GRaph embeddding feature
import networkx as nx
from node2vec import Node2Vec

# Create a sample graph (you can create a graph based on your text data)
G = nx.Graph()
G.add_edge('Barack Obama', 'President')
G.add_edge('Barack Obama', 'United States')
G.add_edge('United States', 'President')

# Generate walks
node2vec = Node2Vec(G, dimensions=64, walk_length=30, num_walks=200, workers=4)

# Learn embeddings
model = node2vec.fit(window=10, min_count=1, batch_words=4)

# Get the embeddings for a specific node (e.g., 'Barack Obama')
embedding = model.wv['Barack Obama']

print("Embedding for 'Barack Obama':", embedding)


# cosine similiarity 
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity

# Sample texts
text1 = "This is a sample text."
text2 = "Here is another sample text."

# Create TF-IDF vectorizer
vectorizer = TfidfVectorizer()
tfidf_matrix = vectorizer.fit_transform([text1, text2])

# Compute cosine similarity
similarity = cosine_similarity(tfidf_matrix[0:1], tfidf_matrix)[0][1]

print("Cosine Similarity:", similarity)
### jaccard similiarity
from sklearn.feature_extraction.text import CountVectorizer

# Sample texts
text1 = "This is a sample text."
text2 = "Here is another sample text."

# Tokenize texts
vectorizer = CountVectorizer()
X = vectorizer.fit_transform([text1, text2])
tokens = vectorizer.get_feature_names()

# Compute Jaccard similarity
intersection = len(set(tokens[0]).intersection(set(tokens[1])))
union = len(set(tokens[0]).union(set(tokens[1])))
jaccard_similarity = intersection / union

print("Jaccard Similarity:", jaccard_similarity)
# ecudlian disatane
from sklearn.feature_extraction.text import CountVectorizer
from scipy.spatial import distance

# Sample texts
text1 = "This is a sample text."
text2 = "Here is another sample text."

# Tokenize texts
vectorizer = CountVectorizer()
X = vectorizer.fit_transform([text1, text2])
tokens = vectorizer.get_feature_names()

# Compute Euclidean distance
vector1 = X.toarray()[0]
vector2 = X.toarray()[1]
euclidean_distance = distance.euclidean(vector1, vector2)

print("Euclidean Distance:", euclidean_distance)
